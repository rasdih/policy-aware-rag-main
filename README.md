# ğŸ›¡ï¸ LLM Prompt Injection Guard (RAG-Based)

A **Retrieval-Augmented Prompt Injection Guard** that detects and prevents malicious prompts targeting Large Language Models by grounding decisions in explicit security policies.

This system combines semantic retrieval, policy-aware reasoning, and enforcement control to create an interpretable and extensible guardrail architecture for LLM applications.

---

## ğŸš€ Motivation

Modern LLM systems are vulnerable to prompt injection attacks such as:

* instruction override
* role escalation
* system prompt extraction
* memory leakage
* tool invocation abuse
* data exfiltration

Most defenses rely on static heuristics or model behavior constraints.

This project explores a **policy-grounded RAG architecture** where security decisions are:

* explainable
* auditable
* extensible
* decoupled from model internals

---

## ğŸ§  System Architecture

```
Policy Corpus
   â†“
Semantic Chunking
   â†“
Embedding Memory
   â†“
Policy Retrieval
   â†“
Policy Evaluator (LLM reasoning)
   â†“
Enforcement Engine
   â†“
Secure Runtime Generation
   â†“
Observability Dashboard
```

### Key Idea

Security policies are treated as **retrievable knowledge**, allowing the system to dynamically reason about violations rather than rely on static filtering.

---

## âœ¨ Features

### ğŸ” Policy-Grounded Detection

Retrieves relevant security policies before evaluating prompts.

### ğŸ§  LLM-Based Policy Reasoning

Uses an LLM to classify violations using retrieved policy context.

### ğŸš« Enforcement Layer

Maps policy violations to runtime actions:

| Decision | Behavior             |
| -------- | -------------------- |
| allow    | normal generation    |
| warn     | generation + notice  |
| restrict | sanitized generation |
| block    | refusal              |

### âš¡ Streaming Secure Responses

Token streaming enabled for permitted generations.

### ğŸ“Š Evaluation Framework

Includes dataset-based evaluation measuring:

* detection accuracy
* precision / recall
* false positive rate
* policy attribution accuracy

### ğŸ–¥ï¸ Guard Dashboard

Interactive Streamlit interface exposing:

* prompt
* retrieved policies
* reasoning
* risk classification
* final response

---

## ğŸ“ Project Structure

```
LLM-Prompt-Injection-Guard/
â”‚
â”œâ”€â”€ policies/                # Security policy corpus
â”œâ”€â”€ rag_guard/
â”‚   â”œâ”€â”€ ingestion/           # Policy chunking & embedding
â”‚   â”œâ”€â”€ retrieval/           # Semantic policy retrieval
â”‚   â”œâ”€â”€ evaluation/          # Policy reasoning evaluator
â”‚   â”œâ”€â”€ enforcement/         # Decision enforcement engine
â”‚   â”œâ”€â”€ runtime/             # Secure generation wrapper
â”‚   â”œâ”€â”€ pipeline/            # End-to-end guard pipeline
â”‚   â””â”€â”€ observability/       # Guard event logging
â”‚
â”œâ”€â”€ evaluation/              # Dataset & evaluation scripts
â”œâ”€â”€ app.py             # Streamlit guard dashboard
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

### 1. Create environment

```
python -m venv .venv
```

Activate:

```
.venv\Scripts\activate
```

### 2. Install dependencies

```
pip install streamlit langchain langchain-core langchain-community langchain-ollama chromadb pandas
```

### 3. Start Ollama

Ensure local Ollama server is running:

```
ollama serve
```

Pull models:

```
ollama pull gemma3
ollama pull mxbai-embed-large
```

---

## â–¶ï¸ Usage

### Build policy memory

```
python -m rag_guard.ingestion.build_index
```

### Run dashboard

```
streamlit run app.py
```

### Evaluate guard

```
python -m rag_guard.evaluation.run_eval
```

---

## ğŸ“Š Evaluation

Evaluation uses a curated adversarial dataset covering:

* system prompt extraction
* jailbreak attempts
* role escalation
* memory leakage
* sensitive data exposure

Metrics computed:

* accuracy
* precision
* recall
* F1 score
* false positive rate
* policy attribution accuracy

---

## ğŸ”¬ Research Insights

This project demonstrates:

* RAG can be applied to **security policy grounding**
* policy retrieval improves interpretability of guard decisions
* separation of detection and enforcement improves modularity
* guard-controlled generation is critical for real defense

---

## ğŸš§ Future Work

* adversarial prompt generation
* multi-step attack simulation
* tool-level runtime guard
* dynamic policy updates
* policy conflict resolution
* guard confidence calibration

---

## ğŸ“œ License

MIT License
